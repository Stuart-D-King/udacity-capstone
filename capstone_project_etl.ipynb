{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top of the Lake\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "Combining immigration, travel, weather, and demographics data sources to create a data model for further analysis into foreign tourism to the United States.\n",
    "\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and installs\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import configparser\n",
    "import datetime\n",
    "import glob\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import types as t\n",
    "from collections import Counter\n",
    "from pyspark.sql import DataFrame\n",
    "from functools import reduce\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Left align tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table {\n",
       "        display: inline-block\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "    table {\n",
    "        display: inline-block\n",
    "    }\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "The project combines four datasets covering immigration and travel, weather, and city demographics.\n",
    "- [I94 Immigration Data](https://travel.trade.gov/research/reports/i94/historical/2016.html): 2016 Immigration data to the United States from the U.S. National Tourism and Trade Office\n",
    "- [World Temperature Data](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data): Earth surface temperature data from Kaggle\n",
    "- [U.S. City Demographic Data](https://public.opendatasoft.com/explore/dataset/us-cities-demographics/information/): Demographics of all U.S. cities and census-designated places with a population greater or equal to 65,000\n",
    "- [Airport Code Table](https://datahub.io/core/airport-codes#data): Airport codes and their corresponding cities\n",
    "\n",
    "Data is modeled using a **data lake** hosted on Amazon Web Services (AWS). Data is first loaded and processed into dimension and analytics tables using Spark and **schema-on-read**. The data is then saved to S3 as partitioned `parquet` files to be easily loaded back into Spark dataframe objects on demand. For the saved analytics tables, a **snowflake schema** of one fact table with accompanying dimension tables is used. The Spark process is deployed on an AWS EMR cluster.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Initial steps include gathering the datasets and performing a high-level review of the data and quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in sample immigration data to scope the full dataset\n",
    "df = pd.read_csv('data/immigration_data_sample.csv', index_col=0).reset_index(drop=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in airport data for initial review\n",
    "df_air = pd.read_csv('data/airport-codes_csv.csv')\n",
    "df_air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_air.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in temperature data for initial review\n",
    "df_temp = pd.read_csv('data/GlobalLandTemperaturesByCity.csv.gz', compression='gzip')\n",
    "df_temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in city demographics data for initial review\n",
    "df_demo = pd.read_csv('data/us-cities-demographics.csv', sep=';')\n",
    "df_demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "\n",
    "Identify data quality issues, like missing values, duplicate data, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate a SparkSession object\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11,org.apache.hadoop:hadoop-aws:2.7.3,com.amazonaws:aws-java-sdk-pom:1.10.34\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Explore and clean the immigration dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- cicid: string (nullable = true)\n",
      " |-- i94yr: string (nullable = true)\n",
      " |-- i94mon: string (nullable = true)\n",
      " |-- i94cit: string (nullable = true)\n",
      " |-- i94res: string (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: string (nullable = true)\n",
      " |-- i94mode: string (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: string (nullable = true)\n",
      " |-- i94bir: string (nullable = true)\n",
      " |-- i94visa: string (nullable = true)\n",
      " |-- count: string (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: string (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: string (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in sample immigration data to a Spark dataframe\n",
    "df_imm = spark.read.csv('data/immigration_data_sample.csv', header=True)\n",
    "df_imm.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following will read in all 2016 I94 immigration data and union each month to create a combined, full-year Spark dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(filepath):\n",
    "    all_files = []\n",
    "    for root, dirs, files in os.walk(filepath):\n",
    "        files = glob.glob(os.path.join(root,'*.sas7bdat'))\n",
    "        for fpath in files :\n",
    "            all_files.append(os.path.abspath(fpath))\n",
    "    \n",
    "    return all_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sas_dir = '../../data/18-83510-I94-Data-2016'\n",
    "sas_filepaths = get_files(sas_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for fp in sas_filepaths:\n",
    "    df = spark.read.format('com.github.saurfang.sas.spark') \\\n",
    "        .load('../..' + fp) \\\n",
    "        .select('i94mon', 'i94cit', 'i94port', 'arrdate', 'depdate', \\\n",
    "                'i94mode', 'i94bir', 'i94visa', 'gender', 'airline', 'visatype')\n",
    "    frames.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imm = reduce(DataFrame.union, frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions to be used in Spark UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date(d):\n",
    "    '''\n",
    "    Convert SAS date to datetime object\n",
    "\n",
    "    INPUT\n",
    "    d: number of days\n",
    "\n",
    "    OUTPUT\n",
    "    Datetime object\n",
    "    '''\n",
    "    epoch = datetime.datetime(1960, 1, 1)\n",
    "    if d is not None:\n",
    "        return epoch + datetime.timedelta(days=d)\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "date_func = f.udf(lambda d: convert_date(d), t.DateType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_trip_purpose(tp):\n",
    "    '''\n",
    "    Map trip purpose codes to strings\n",
    "\n",
    "    INPUT\n",
    "    tp: integer identifier of trip purpose\n",
    "\n",
    "    OUTPUT\n",
    "    String identifier of trip purpose\n",
    "    '''\n",
    "    if tp is not None:\n",
    "        if tp == 1:\n",
    "            return 'Business'\n",
    "        elif tp == 2:\n",
    "            return 'Pleasure'\n",
    "        elif tp == 3:\n",
    "            return 'Student'\n",
    "    else:\n",
    "        None\n",
    "        \n",
    "trip_purpose_func = f.udf(lambda tp: map_trip_purpose(tp), t.StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UDFs to convert lists to dictionary counter objects\n",
    "int_counter_udf = f.udf(lambda s: dict(Counter(s)), t.MapType(t.IntegerType(), t.IntegerType()))\n",
    "str_counter_udf = f.udf(lambda s: dict(Counter(s)), t.MapType(t.StringType(), t.IntegerType())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_median(lst):\n",
    "    '''\n",
    "    Calculate median from list of values\n",
    "\n",
    "    INPUT\n",
    "    lst: list of values\n",
    "\n",
    "    OUTPUT\n",
    "    Calculated median\n",
    "    '''\n",
    "    med = np.median(lst)\n",
    "    return float(med)\n",
    "\n",
    "median_func = f.udf(calc_median, t.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check null counts by variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>_c0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>cicid</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94yr</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94mon</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94cit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94res</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94port</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>arrdate</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94mode</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94addr</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>depdate</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94bir</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>i94visa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dtadfile</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>visapost</td>\n",
       "      <td>618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>occup</td>\n",
       "      <td>996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>entdepa</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>entdepd</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>entdepu</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>matflag</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>biryear</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>dtaddto</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>gender</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>insnum</td>\n",
       "      <td>965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>airline</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>admnum</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>fltno</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>visatype</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0\n",
       "_c0          0\n",
       "cicid        0\n",
       "i94yr        0\n",
       "i94mon       0\n",
       "i94cit       0\n",
       "i94res       0\n",
       "i94port      0\n",
       "arrdate      0\n",
       "i94mode      0\n",
       "i94addr     59\n",
       "depdate     49\n",
       "i94bir       0\n",
       "i94visa      0\n",
       "count        0\n",
       "dtadfile     0\n",
       "visapost   618\n",
       "occup      996\n",
       "entdepa      0\n",
       "entdepd     46\n",
       "entdepu   1000\n",
       "matflag     46\n",
       "biryear      0\n",
       "dtaddto      0\n",
       "gender     141\n",
       "insnum     965\n",
       "airline     33\n",
       "admnum       0\n",
       "fltno        8\n",
       "visatype     0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_imm.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in df_imm.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed the following transformations on the immigration dataset:\n",
    "- Selected and cast variables to correct data types\n",
    "- Dropped duplicate records\n",
    "- Filterd to air entry mode only\n",
    "- Remobed unknown/unidentified entry ports\n",
    "- Converted arrival and departure variables to date fields before creating a length of stay variable\n",
    "- Converted trip purpose to a string field\n",
    "- Replaced 'X' in the gender field with None\n",
    "- Dropped fields no longer needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm_table = df_imm \\\n",
    "    .filter(f.col('i94mode') == 1) \\\n",
    "    .filter(~f.col('i94port').isin(['XXX', '888', 'UNK'])) \\\n",
    "    .select(\n",
    "        f.col('i94mon').cast(t.IntegerType()).alias('month'),\n",
    "        f.col('i94cit').cast(t.IntegerType()).alias('country_id'),\n",
    "        f.col('i94port').alias('airport_cd'),\n",
    "        f.col('arrdate').cast(t.IntegerType()).alias('arrival_date'), \n",
    "        f.col('depdate').cast(t.IntegerType()).alias('departure_date'),\n",
    "        f.col('i94bir').cast(t.IntegerType()).alias('age'),\n",
    "        f.col('i94visa').cast(t.IntegerType()).alias('trip_purpose'),\n",
    "        f.col('gender'),\n",
    "        f.col('airline').alias('airline_cd'),\n",
    "        f.col('visatype').alias('visa_type')) \\\n",
    "    .dropDuplicates() \\\n",
    "    .withColumn('arrival_date', date_func(f.col('arrival_date'))) \\\n",
    "    .withColumn('departure_date', date_func(f.col('departure_date'))) \\\n",
    "    .withColumn('trip_purpose', trip_purpose_func(f.col('trip_purpose'))) \\\n",
    "    .withColumn('length_of_stay', f.datediff(f.col('departure_date'), f.col('arrival_date'))) \\\n",
    "    .withColumn('gender', f.when(f.col('gender') == 'X', None).otherwise(f.col('gender'))) \\\n",
    "    .drop('arrival_date', 'departure_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data has been aggregated by month and airport. When appropriate MapType field structuring is used to further segment aggregations by variable. For example, the gender field has been converted to a MapType in which Male and Female are keys and visit counts are values. Other aggregations include the average length of stay and the median visitor age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENDER counts by month and airport\n",
    "# imm_table.filter(f.col('gender').isNotNull()) \\\n",
    "#     .groupBy('year', 'month', 'entry_port_cd') \\\n",
    "#     .pivot('gender') \\\n",
    "#     .agg(f.countDistinct('immigration_id')) \\\n",
    "#     .fillna(0) \\\n",
    "#     .withColumnRenamed('F', 'female_cnt') \\\n",
    "#     .withColumnRenamed('M', 'male_cnt') \\\n",
    "#     .show(5)\n",
    "\n",
    "gender_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.collect_list('gender').alias('gender')) \\\n",
    "    .withColumn('gender', str_counter_udf(f.col('gender'))) \\\n",
    "    .alias('gender_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AVG LENGTH OF STAY by month and airport\n",
    "stay_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.round(f.mean('length_of_stay'),2).alias('avg_stay')) \\\n",
    "    .alias('stay_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MEDIAN AGE by month and airport\n",
    "age_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(median_func(f.collect_list(f.col('age'))).alias('median_age')) \\\n",
    "    .alias('age_table')\n",
    "\n",
    "# imm_table.groupBy('year', 'month', 'entry_port_cd').agg(f.round(f.mean('age'),2).alias('avg_age')).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRIP PURPOSE countes by month and airport\n",
    "# imm_table.groupBy('year', 'month', 'entry_port_cd') \\\n",
    "#     .pivot('trip_purpose') \\\n",
    "#     .agg(f.count('immigration_id')) \\\n",
    "#     .fillna(0) \\\n",
    "#     .withColumnRenamed('Business', 'business_trip_cnt') \\\n",
    "#     .withColumnRenamed('Pleasure', 'pleasure_trip_cnt') \\\n",
    "#     .withColumnRenamed('Student', 'student_trip_cnt') \\\n",
    "#     .show(5)\n",
    "\n",
    "trip_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.collect_list('trip_purpose').alias('trip_purpose')) \\\n",
    "    .withColumn('trip_purpose', str_counter_udf(f.col('trip_purpose'))) \\\n",
    "    .alias('trip_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISA TYPE counts by month and airport\n",
    "visa_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.collect_list('visa_type').alias('visa_type')) \\\n",
    "    .withColumn('visa_type', str_counter_udf(f.col('visa_type'))) \\\n",
    "    .alias('visa_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTRY counts by month and airport\n",
    "country_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.collect_list('country_id').alias('countries')) \\\n",
    "    .withColumn('countries', int_counter_udf(f.col('countries'))) \\\n",
    "    .alias('country_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIRLINE counts by month and airport\n",
    "airline_table = imm_table.groupBy('month', 'airport_cd') \\\n",
    "    .agg(f.collect_list('airline_cd').alias('airlines')) \\\n",
    "    .withColumn('airlines', str_counter_udf(f.col('airlines'))) \\\n",
    "    .alias('airline_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOTAL count by month and airport\n",
    "cnt_table = imm_table.groupBy('month', 'airport_cd').count().alias('cnt_table')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After aggregating immigration data, each individual grouping is combined into one analytical table. The Spark `monotonically_increasing_id` function is used to create a unique primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- airport_cd: string (nullable = true)\n",
      " |-- gender: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- avg_stay: double (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- trip_purpose: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- visa_type: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- countries: map (nullable = true)\n",
      " |    |-- key: integer\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- airlines: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: integer (valueContainsNull = true)\n",
      " |-- immigration_id: long (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full immigration table\n",
    "immigration_table = cnt_table \\\n",
    "    .join(gender_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('gender_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('gender_table.airport_cd')], how='left') \\\n",
    "    .join(stay_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('stay_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('stay_table.airport_cd')], how='left') \\\n",
    "    .join(age_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('age_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('age_table.airport_cd')], how='left') \\\n",
    "    .join(trip_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('trip_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('trip_table.airport_cd')], how='left') \\\n",
    "    .join(visa_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('visa_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('visa_table.airport_cd')], how='left') \\\n",
    "    .join(country_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('country_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('country_table.airport_cd')], how='left') \\\n",
    "    .join(airline_table, on=[\n",
    "        f.col('cnt_table.month') == f.col('airline_table.month'), \\\n",
    "        f.col('cnt_table.airport_cd') == f.col('airline_table.airport_cd')], how='left') \\\n",
    "    .select(\n",
    "        cnt_table.month,\n",
    "        cnt_table.airport_cd,\n",
    "        gender_table.gender,\n",
    "        stay_table.avg_stay,\n",
    "        age_table.median_age,\n",
    "        trip_table.trip_purpose,\n",
    "        visa_table.visa_type,\n",
    "        country_table.countries,\n",
    "        airline_table.airlines) \\\n",
    "    .withColumn('immigration_id', f.monotonically_increasing_id())\n",
    "\n",
    "immigration_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Explore and clean the airport dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- continent: string (nullable = true)\n",
      " |-- iso_country: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- coordinates: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in airport data to a Spark dataframe (define schema or infer schema)\n",
    "df_air = spark.read.csv('data/airport-codes_csv.csv', header=True)\n",
    "df_air.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|          type|\n",
      "+--------------+\n",
      "| large_airport|\n",
      "|medium_airport|\n",
      "| small_airport|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_air.select('type').where(f.col('type').like('%airport%')).distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed the following transformations on the airport dataset:\n",
    "- Dropping rows with null IATA code\n",
    "- Filtered to U.S. airports\n",
    "- Split the coordinates field to create separate latitude and longitude fields\n",
    "- Extracted the state code from the iso_region field by splitting on a hyphen\n",
    "- Selected and cast needed variables\n",
    "- Dropped duplicate records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- airport_cd: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "airport_table = df_air.na.drop(subset=['iata_code']) \\\n",
    "    .filter(f.col('iso_country') == 'US') \\\n",
    "    .where(f.col('type').like('%airport%')) \\\n",
    "    .withColumn('coord_split', f.split(f.col('coordinates'), ',')) \\\n",
    "    .withColumn('state_split', f.split(f.col('iso_region'), '-')) \\\n",
    "    .select(\n",
    "        f.col('iata_code').alias('airport_cd'),\n",
    "        f.col('type'),\n",
    "        f.col('name'),\n",
    "        f.col('state_split')[1].alias('state'),\n",
    "        f.col('municipality').alias('city'),\n",
    "        f.col('coord_split')[1].cast(t.DoubleType()).alias('latitude'),\n",
    "        f.col('coord_split')[0].cast(t.DoubleType()).alias('longitude')) \\\n",
    "    .dropDuplicates() \\\n",
    "    .na.drop(subset=['city'])\n",
    "\n",
    "airport_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Explore and clean the city demographics dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in city demographics data to a Spark dataframe (define schema or infer schema)\n",
    "df_demo = spark.read.csv('data/us-cities-demographics.csv', header=True, sep=';')\n",
    "df_demo.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|                Race|\n",
      "+--------------------+\n",
      "|Black or African-...|\n",
      "|  Hispanic or Latino|\n",
      "|               White|\n",
      "|               Asian|\n",
      "|American Indian a...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_demo.select('Race').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_demo.groupBy('City').pivot('Race').agg({'Count':'sum'}).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selected, cast, and renamed the desired data fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city = df_demo.select(\n",
    "        f.col('City').alias('city'),\n",
    "        f.col('State Code').alias('state'),\n",
    "        f.col('Median Age').cast(t.DoubleType()).alias('median_age'),\n",
    "        f.col('Male Population').cast(t.IntegerType()).alias('male_pop'),\n",
    "        f.col('Female Population').cast(t.IntegerType()).alias('female_pop'),\n",
    "        f.col('Total Population').cast(t.IntegerType()).alias('total_pop'),\n",
    "        f.col('Number of Veterans').cast(t.IntegerType()).alias('veterans'),\n",
    "        f.col('Foreign-born').cast(t.IntegerType()).alias('foreign_born'),\n",
    "        f.col('Average Household Size').cast(t.DoubleType()).alias('avg_hh_size')) \\\n",
    "    .dropDuplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created a pivot table on race, grouping by city and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_city_pivot = df_demo.groupBy('City', 'State Code').pivot('Race').agg({'Count': 'sum'}) \\\n",
    "    .withColumnRenamed('City', 'city') \\\n",
    "    .withColumnRenamed('State Code', 'state') \\\n",
    "    .withColumnRenamed('American Indian and Alaska Native', 'native_am_pop') \\\n",
    "    .withColumnRenamed('Asian', 'asian_pop') \\\n",
    "    .withColumnRenamed('Black or African-American', 'black_pop') \\\n",
    "    .withColumnRenamed('Hispanic or Latino', 'hispanic_pop') \\\n",
    "    .withColumnRenamed('White', 'white_pop')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merged the two tables to create one dimension table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- median_age: double (nullable = true)\n",
      " |-- male_pop: integer (nullable = true)\n",
      " |-- female_pop: integer (nullable = true)\n",
      " |-- native_am_pop: integer (nullable = true)\n",
      " |-- asian_pop: integer (nullable = true)\n",
      " |-- black_pop: integer (nullable = true)\n",
      " |-- hispanic_pop: integer (nullable = true)\n",
      " |-- white_pop: integer (nullable = true)\n",
      " |-- veterans: integer (nullable = true)\n",
      " |-- foreign_born: integer (nullable = true)\n",
      " |-- avg_hh_size: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "demo_table = df_city.join(df_city_pivot, \n",
    "                          on=[df_city.city == df_city_pivot.city, df_city.state == df_city_pivot.state]) \\\n",
    "    .select(df_city.city,\n",
    "        df_city.state,\n",
    "        df_city.median_age,\n",
    "        df_city.male_pop,\n",
    "        df_city.female_pop,\n",
    "        df_city_pivot.native_am_pop.cast(t.IntegerType()),\n",
    "        df_city_pivot.asian_pop.cast(t.IntegerType()),\n",
    "        df_city_pivot.black_pop.cast(t.IntegerType()),\n",
    "        df_city_pivot.hispanic_pop.cast(t.IntegerType()),\n",
    "        df_city_pivot.white_pop.cast(t.IntegerType()),\n",
    "        df_city.veterans,\n",
    "        df_city.foreign_born,\n",
    "        df_city.avg_hh_size)\n",
    "\n",
    "demo_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Explore and clean the global land temperature by city dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- dt: string (nullable = true)\n",
      " |-- AverageTemperature: string (nullable = true)\n",
      " |-- AverageTemperatureUncertainty: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in temperature data to a Spark dataframe (define schema or infer schema)\n",
    "df_temp = spark.read.csv('data/GlobalLandTemperaturesByCity.csv.gz', header=True)\n",
    "df_temp.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for fields with null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>dt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AverageTemperature</td>\n",
       "      <td>364130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>AverageTemperatureUncertainty</td>\n",
       "      <td>364130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>City</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Country</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Latitude</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Longitude</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    0\n",
       "dt                                  0\n",
       "AverageTemperature             364130\n",
       "AverageTemperatureUncertainty  364130\n",
       "City                                0\n",
       "Country                             0\n",
       "Latitude                            0\n",
       "Longitude                           0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.select([f.count(f.when(f.col(c).isNull(), c)).alias(c) for c in df_temp.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created helper functions to be used in Spark UDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cel_to_fah(c):\n",
    "    '''\n",
    "    Convert celsius to fahrenheit\n",
    "\n",
    "    INPUT\n",
    "    c: integer value for temperature in Celsius\n",
    "\n",
    "    OUTPUT\n",
    "    Float value for temperature in Fahrenheit\n",
    "    '''\n",
    "    if c is not None:\n",
    "        return c*(9/5)+32\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "temp_conver_func = f.udf(lambda c: cel_to_fah(c), t.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_latlong(c):\n",
    "    '''\n",
    "    Convert cardinal directions to +/- latitude and longitude coordinates\n",
    "\n",
    "    INPUT\n",
    "    c: latitude or longitude coordinate with cardinal direction as the last character\n",
    "\n",
    "    OUTPUT\n",
    "    Postive or negative coordinate value\n",
    "    '''\n",
    "    if c is not None:\n",
    "        num = c[:-1]\n",
    "        quad = c[-1]\n",
    "        if quad.upper() in ('S', 'W'):\n",
    "            return float(num) * -1\n",
    "        elif quad.upper() in ('N', 'E'):\n",
    "            return float(num)\n",
    "        else:\n",
    "            None\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "latlong_func = f.udf(lambda c: convert_latlong(c), t.DoubleType())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most recent year of data available is 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(max(year)=2013)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_temp.filter(f.col('Country') == 'United States') \\\n",
    "    .withColumn('dim_date', f.to_date(f.col('dt'))) \\\n",
    "    .withColumn('year', f.year('dim_date')) \\\n",
    "    .agg({'year': 'max'}).collect()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performed the following transformations to the temperature dataset:\n",
    "- Filtered to U.S. records\n",
    "- Converted the `dt` field to datetime\n",
    "- Filtered to records since 1950\n",
    "- Dropped any records missing an average temperature\n",
    "- Selected, cast, and renamed useful fields\n",
    "- Converted the average temperature field from Celsius to Fahrenheit\n",
    "- Dropped duplicate records\n",
    "- Calculated the median average temperature by month, city, and lat/long coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- month: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- median_avg_temp: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_table = df_temp.filter(f.col('Country') == 'United States') \\\n",
    "    .withColumn('dim_date', f.to_date(f.col('dt'))) \\\n",
    "    .filter(f.year(f.col('dim_date')) >= 1950) \\\n",
    "    .na.drop(subset=['AverageTemperature']) \\\n",
    "    .select(\n",
    "        f.month(f.col('dim_date')).alias('month'),\n",
    "        f.col('AverageTemperature').cast(t.DoubleType()).alias('avg_temp'),\n",
    "        f.col('City').alias('city'),\n",
    "        f.col('Latitude').alias('latitude'),\n",
    "        f.col('Longitude').alias('longitude')) \\\n",
    "    .withColumn('avg_temp', f.round(temp_conver_func(f.col('avg_temp')), 2)) \\\n",
    "    .withColumn('latitude', latlong_func(f.col('latitude'))) \\\n",
    "    .withColumn('longitude', latlong_func(f.col('longitude'))) \\\n",
    "    .dropDuplicates() \\\n",
    "    .groupBy('month', 'city', 'latitude', 'longitude') \\\n",
    "    .agg(median_func(f.collect_list(f.col('avg_temp'))).alias('median_avg_temp'))\n",
    "\n",
    "temp_table.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "A **data lake** using a snowflake schema-on-read approach is utilized due to the multiple levels of relationships between dimension tables. From a centralized immigration fact table, airport, city demographics, and temperature dimension tables are interconnected, forming one-to-many relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fact Table\n",
    "**immigration**\n",
    "- immigration_id: long\n",
    "- month: integer\n",
    "- airport_cd: string\n",
    "- gender: map\n",
    "    - key: string\n",
    "    - value: integer\n",
    "- avg_stay: double\n",
    "- median_age: double\n",
    "- trip_purpose: map\n",
    "    - key: string\n",
    "    - value: integer\n",
    "- visa_type: map\n",
    "    - key: string\n",
    "    - value: integer\n",
    "- countries: map\n",
    "    - key: integer\n",
    "    - value: integer\n",
    "- airlines: map\n",
    "    - key: string\n",
    "    - value: integer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dimension Tables\n",
    "**airport**\n",
    "- airport_cd: string\n",
    "- type: string\n",
    "- name: string\n",
    "- state: string\n",
    "- city: string\n",
    "- latitude: double\n",
    "- longitude: double\n",
    "    \n",
    "**demographics**\n",
    "- city: string\n",
    "- state: string\n",
    "- median_age: double\n",
    "- male_pop: integer\n",
    "- female_pop: integer\n",
    "- native_am_pop: integer\n",
    "- asian_pop: integer\n",
    "- black_pop: integer\n",
    "- hispanic_pop: integer\n",
    "- white_pop: integer\n",
    "- veterans: integer\n",
    "- foreign_born: integer\n",
    "- avg_hh_size: double\n",
    "    \n",
    "**temperature** \n",
    "- month: integer  \n",
    "- city: string  \n",
    "- latitude: double  \n",
    "- longitude: double  \n",
    "- median_avg_temp: double  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![data-model](img/udacity_capstone_data_model.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Mapping Out Data Pipelines\n",
    "The data pipeline is centured around Spark and AWS EMR. The pipeline consists of the follwoing steps:\n",
    "1. Using Infrastructure-as-Code (IaC), create a new EMR cluster with Spark installed\n",
    "2. Copy the ETL pipeline from S3 onto the EMR cluster\n",
    "3. Submit the ETL pipeline to `spark-submit` to perform data transformations and save output tables to S3\n",
    "4. Terminate the EMR cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Run ETL to Model the Data\n",
    "#### 4.1 Move data files to S3\n",
    "Copy raw data files and the `etl.py` script to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% python copy_to_s3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Run Pipeline\n",
    "Create an EMR cluster, move the `etl.py` pipeline script from S3 onto the cluster, execute the pipeline using `spark-submit`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% python emr.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Data Quality Checks\n",
    "The following data quality checks confirm data completeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Immigration table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>airport_cd</th>\n",
       "      <th>gender</th>\n",
       "      <th>avg_stay</th>\n",
       "      <th>median_age</th>\n",
       "      <th>trip_purpose</th>\n",
       "      <th>visa_type</th>\n",
       "      <th>countries</th>\n",
       "      <th>airlines</th>\n",
       "      <th>immigration_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  airport_cd  gender  avg_stay  median_age  trip_purpose  visa_type  \\\n",
       "0      0           0       0         1           0             0          0   \n",
       "\n",
       "   countries  airlines  immigration_id  \n",
       "0          0         0               0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null counts\n",
    "immigration_table.select([f.count(f.when(f.col(c).isNull(), c)) \\\n",
    "                          .alias(c) for c in immigration_table.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of duplicate airports\n",
    "immigration_table.groupBy(\"airport_cd\").count().filter(\"count >= 2\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Airports table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airport_cd</th>\n",
       "      <th>type</th>\n",
       "      <th>name</th>\n",
       "      <th>state</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   airport_cd  type  name  state  city  latitude  longitude\n",
       "0           0     0     0      0     0         0          0"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null counts\n",
    "airport_table.select([f.count(f.when(f.col(c).isNull(), c)) \\\n",
    "                          .alias(c) for c in airport_table.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1865"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of airports\n",
    "airport_table.select(f.col('airport_cd')).distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of duplicate airports\n",
    "airport_table.groupBy(\"airport_cd\").count().filter(\"count >= 2\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*City demographics table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>median_age</th>\n",
       "      <th>male_pop</th>\n",
       "      <th>female_pop</th>\n",
       "      <th>native_am_pop</th>\n",
       "      <th>asian_pop</th>\n",
       "      <th>black_pop</th>\n",
       "      <th>hispanic_pop</th>\n",
       "      <th>white_pop</th>\n",
       "      <th>veterans</th>\n",
       "      <th>foreign_born</th>\n",
       "      <th>avg_hh_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>13</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   city  state  median_age  male_pop  female_pop  native_am_pop  asian_pop  \\\n",
       "0     0      0           0         1           1             57         13   \n",
       "\n",
       "   black_pop  hispanic_pop  white_pop  veterans  foreign_born  avg_hh_size  \n",
       "0         12             0          7         7             7            8  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null counts\n",
    "demo_table.select([f.count(f.when(f.col(c).isNull(), c)) \\\n",
    "                          .alias(c) for c in demo_table.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype of city field == string\n",
    "demo_table.schema['city'].dataType == t.StringType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Temperature table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>month</th>\n",
       "      <th>city</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>median_avg_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   month  city  latitude  longitude  median_avg_temp\n",
       "0      0     0         0          0                0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null counts\n",
    "temp_table.select([f.count(f.when(f.col(c).isNull(), c)) \\\n",
    "                          .alias(c) for c in temp_table.columns]).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 12 months\n",
    "temp_table.select(f.col('month')).distinct().count() == 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dtype of median_avg_temp field == string\n",
    "temp_table.schema['median_avg_temp'].dataType == t.DoubleType()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Joining of immigration table with airport table*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "29"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "immigration_table.join(airport_table, \n",
    "                        on=[immigration_table.airport_cd == airport_table.airport_cd]) \\\n",
    "    .select(\n",
    "        immigration_table.month,\n",
    "        airport_table.airport_cd) \\\n",
    "    .distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`immigration`\n",
    "\n",
    "| column name    | data type | description | origin |\n",
    "| -------------- | --------- | ----------- | ------ |\n",
    "| immigration_id | long | monotonically increasing and unique ID | PySpark generated |\n",
    "| month | integer | integer representation of month of the year | 2016 I-94 immigration data |\n",
    "| airport_cd | string | IATA airport code | 2016 I-94 immigration data |\n",
    "| gender | map (key: string, value: integer) | visit counts by gender in key value pairs | 2016 I-94 immigration data |\n",
    "| avg_stay | double | average length of stay in days | 2016 I-94 immigration data |\n",
    "| median_age | double | median age of visitors | 2016 I-94 immigration data |\n",
    "| trip_purpose | map (key: string, value: integer) | visit counts by trip purpose in key value pairs | 2016 I-94 immigration data |\n",
    "| visa_type | map (key: string, value: integer) | visit counts by visa type in key value pairs | 2016 I-94 immigration data |\n",
    "| countries | map (key: string, value: integer) | visit counts by country of origin in key value pairs | 2016 I-94 immigration data |\n",
    "| airlines | map (key: string, value: integer) | visit counts by airline in key value pairs | 2016 I-94 immigration data |\n",
    "\n",
    "\n",
    "`airport`\n",
    "\n",
    "| column name    | data type | description | origin |\n",
    "| -------------- | --------- | ----------- | ------ |\n",
    "| airport_cd | string | IATA airport code | airport location information and attributes dataset from datahub.io |\n",
    "| type | string | type of airport (small, medium, large) | airport location information and attributes dataset from datahub.io |\n",
    "| name | string | airport name | airport location information and attributes dataset from datahub.io |\n",
    "| state | string | two letter U.S. state code of airport location | airport location information and attributes dataset from datahub.io |\n",
    "| city | string | U.S. city of airport location | airport location information and attributes dataset from datahub.io |\n",
    "| latitude | double | latitudinal coordinate of airport location | airport location information and attributes dataset from datahub.io |\n",
    "| longitude | double | longitudinal coordinate of airport location | airport location information and attributes dataset from datahub.io |\n",
    "\n",
    "\n",
    "`demographics`\n",
    "\n",
    "| column name    | data type | description | origin |\n",
    "| -------------- | --------- | ----------- | ------ |\n",
    "| city | string | U.S. city name | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| state | string | two letter U.S. state code | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| median_age | double | median age of city resident | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| male_pop | integer | male population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| female_pop | integer | female population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| native_am_pop | integer | native american population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| asian_pop | integer | asian population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| black_pop | integer | african american population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| hispanic_pop | integer | hispanic population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| white_pop | integer | caucasian population | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| veterans | integer | number of veterans | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| foreign_born | integer | number of foreign born residents | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "| avg_hh_size | double | average household size | city demographics data from the U.S. Census Bureau's 2015 American Community Survey |\n",
    "\n",
    "\n",
    "`temperature`\n",
    "\n",
    "| column name    | data type | description | origin |\n",
    "| -------------- | --------- | ----------- | ------ |\n",
    "| month | integer | integer representation of month of the year | 1950-2013 temperature data from the Berkeley Earth Surface Temperature Study |\n",
    "| city | string | U.S. city name | 1950-2013 temperature data from the Berkeley Earth Surface Temperature Study |\n",
    "| latitude | double | latitudinal coordinate of city | 1950-2013 temperature data from the Berkeley Earth Surface Temperature Study |\n",
    "| longitude | double | longitudinal coordinate of city | 1950-2013 temperature data from the Berkeley Earth Surface Temperature Study |\n",
    "| median_avg_temp | double | median average temperature | 1950-2013 temperature data from the Berkeley Earth Surface Temperature Study |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Save Analytics Tables\n",
    "Table output is saved as parquet files to S3 as a last step in the ETL pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_data = 's3n://sking-udacity-capstone/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write immigration table to parquet\n",
    "imm_output = output_data + 'parquet-files/immigration'\n",
    "immigration_table.write.partitionBy('month','airport_cd').parquet(imm_output)\n",
    "immigration_table = spark.read.parquet(imm_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write airport table to parquet files\n",
    "airport_output = output_data + 'parquet-files/airports'\n",
    "airport_table.write.parquet(airport_output)\n",
    "airport_table = spark.read.parquet(airport_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write demographics table to parquet files\n",
    "demo_output = output_data + 'parquet-files/demographics'\n",
    "demo_table.write.partitionBy('city','state').parquet(demo_output)\n",
    "demo_table = spark.read.parquet(demo_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write temperature table to parquet files\n",
    "temp_output = output_data + 'parquet-files/temperature'\n",
    "temp_table.write.partitionBy('month','city').parquet(temp_output)\n",
    "temp_table = spark.read.parquet(temp_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Project Write Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Rationale\n",
    "The project relies heavily on Apache Spark and AWS. A data lake model was chosen for the following reasons:\n",
    "1. On-demand cloud-based technologies are easily scaled and can cut down on processing costs and overhead\n",
    "2. Spark is best-in-class for big data processing, and allows for advanced downstream analytics and machine learning\n",
    "3. Schema-on-read supports the ingestion of all data formats, regardless of perceived value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Refreshing Data\n",
    "Data has been aggregated at the city and month level. As such, it is recommended that data be refreshed and updated on a monthly cadence. As the data is designed to inform marketing and advertising considerations, monthly updates should be sufficient to capture key changes in the underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3 Scenarios\n",
    "_1. The data increased by 100x_  \n",
    "> The selected data model lends itself kindly to sudden and significant increases in data size. Spark's distributed processing engine combined with AWS elastic computing can easily handle an increase in data by 100x.\n",
    "\n",
    "_2. The data populates a daily-updated dashboard_  \n",
    "> The intended purpose of the selected data model is to provide data aggregrated by month and city. However, current month statistics could be recalculated on a daily basis as new data becomes available. The likelihood of new data holding sway over pre-computed statistics is low, but a daily refresh would be fairly straight foward to implement using the the existing data pipeline and a ETL pipelining tool such as Apache Airflow.\n",
    "\n",
    "_3. The database is accessed by 100+ people_  \n",
    "> Because the project employs a data lake model, a relational database is not relevant. The data pipeline will construct analytics tables and then save the output as parquet files to S3. These files can then be accessed by anyone with S3 read permissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
